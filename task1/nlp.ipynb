{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook implements the NLP with Word to Bags. It is divided in two parts: the tool functions and the final main NLP function. The NLP can be done eather on each diagonistic (1,2 and 3) individually and then we combine the vectors or it first combines the text for each diagnostic (1,2 and 3) and then runs NLP. Different types of Word to Bags (simple or advanced) and vocabulary extraction (simple or advanced) can also be run with this NLP function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Functions for tokenizing,  stemming ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_combine(data, mode):\n",
    "    \n",
    "    # mode = 1,2,3 if want one to extract diag 1,2 or 3 separately \n",
    "    # and 0 if extract everything\n",
    "        \n",
    "    # Getting the text columns\n",
    "    diag_1_txt = pd.DataFrame(data,columns=['diag_1_desc'])\n",
    "    diag_2_txt = pd.DataFrame(data,columns=['diag_2_desc'])\n",
    "    diag_3_txt = pd.DataFrame(data,columns=['diag_3_desc'])\n",
    "    \n",
    "    # Extracting text data in correct format\n",
    "    corpus1 = diag_1_txt.diag_1_desc.values.astype('U')\n",
    "    corpus2 = diag_2_txt.diag_2_desc.values.astype('U')\n",
    "    corpus3 = diag_3_txt.diag_3_desc.values.astype('U')\n",
    "       \n",
    "    # Combining the (or not) corpus together\n",
    "    corpus = list()\n",
    "    for i in range(len(corpus1)):\n",
    "        \n",
    "        doc_corpus_1 = \"\"\n",
    "        doc_corpus_3 = \"\"\n",
    "        doc_corpus_2 = \"\"\n",
    "        \n",
    "        if mode == 1 or mode == 0:\n",
    "            doc_corpus_1 = str(corpus1[i])\n",
    "        if mode == 2 or mode == 0:\n",
    "            doc_corpus_2 = str(corpus2[i])\n",
    "        if mode == 3 or mode == 0:\n",
    "            doc_corpus_3 = str(corpus3[i])\n",
    "                \n",
    "        corpus.append(doc_corpus_1 + \" \" + doc_corpus_2 + \" \" + doc_corpus_3)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def clean_document(document, min_word_length):\n",
    "    \n",
    "    # Split into tokens by white space\n",
    "    tokens = document.split()\n",
    "    \n",
    "    # Remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # Remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # Extract the stem of the words\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "\n",
    "    # Filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > min_word_length]\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_document_from_vocabulary(document, vocabulary, min_word_length):\n",
    "    \n",
    "    # Clean the document\n",
    "    tokens = clean_document(document, min_word_length)\n",
    "    \n",
    "    # Keep only words that are in the vocabulary\n",
    "    tokens = [w for w in tokens if w in vocabulary]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_filter_corpus_from_vocabulary(data, vocabulary, mode, min_word_length):\n",
    "    \n",
    "    # Extract corpus from data\n",
    "    extracted_corpus = extract_and_combine(data, mode)\n",
    "    \n",
    "    # Filter all documents in corpus\n",
    "    filtered_corpus = list()\n",
    "    for document in extracted_corpus:\n",
    "        filtered_corpus.append(filter_document_from_vocabulary(document, vocabulary, min_word_length))\n",
    "        \n",
    "    return filtered_corpus\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool functions for vocabulary extraction (simple or advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def construct_simple_vocabulary(data, min_freq_occurence, max_freq_occurence, mode, min_word_length):\n",
    "    \n",
    "    # Vocabulary\n",
    "    vocabulary = Counter()\n",
    "    \n",
    "    # Extract corpus from data\n",
    "    corpus = extract_and_combine(data, mode)\n",
    "    \n",
    "    # Get vocabulary of all documents\n",
    "    for document in corpus:\n",
    "        vocabulary.update(clean_document(document, min_word_length))\n",
    "    \n",
    "    # Lenght of vocabulary\n",
    "    voc_len = len(vocabulary.items())\n",
    "        \n",
    "    # Keep only most frequent words and delete too frequent words \n",
    "    vocabulary = [k for k,c in vocabulary.items() if (c/voc_len >= min_freq_occurence \n",
    "                                                      and c/voc_len <= max_freq_occurence)]\n",
    "        \n",
    "    return vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_advanced_vocabulary(data, threshold_proba_0, threshold_proba_1, \n",
    "                                  min_freq_occurence, max_freq_occurence, mode, min_word_length):\n",
    "    \n",
    "    # Compute document readmitted = 0\n",
    "    readmitted_0 = compute_readmitted_0(data)\n",
    "    \n",
    "    # Extract corpus from data\n",
    "    corpus = extract_and_combine(data, mode)\n",
    "    \n",
    "    # Compute for each word the proportion of admitted = 0 and admitted = 1 and their occurences \n",
    "    word_to_doc_class = compute_word_to_doc_class(corpus, min_word_length, readmitted_0)\n",
    "    \n",
    "    # Compute probability = (#admitted=1)/(nb_doc) and frequency = pourcentage of occurence in each document\n",
    "    (probability, frequency) = compute_probability_and_frequency(word_to_doc_class, len(corpus))\n",
    "    \n",
    "    # Choose the vocabulary depending on probability and frequency\n",
    "    all_words = Counter()\n",
    "    for document in corpus:\n",
    "        all_words.update(clean_document(document, min_word_length))\n",
    "    all_words = list(all_words.keys())\n",
    "        \n",
    "    vocabulary = list()\n",
    "    for i in range(len(all_words)):\n",
    "        \n",
    "        potential_word = all_words[i]\n",
    "               \n",
    "        if probability[i]<0.5:\n",
    "            add_potential_word = frequency[i] < max_freq_occurence and frequency[i] > min_freq_occurence and probability[i]<threshold_proba_0                     \n",
    "        else:\n",
    "            add_potential_word = frequency[i] < max_freq_occurence and frequency[i] > min_freq_occurence and probability[i]>threshold_proba_1    \n",
    "        \n",
    "        if add_potential_word:\n",
    "            vocabulary.append(potential_word)\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_probability_and_frequency(word_to_doc_class, nb_documents):\n",
    "    \n",
    "    # Compute X=probabilty and Y=frequency \n",
    "    probability = list()\n",
    "    frequency = list()\n",
    "    for doc_ids_0, doc_ids_1 in word_to_doc_class.values():\n",
    "        \n",
    "        totl = len(doc_ids_0) + len(doc_ids_1) \n",
    "        frequency.append(totl/nb_documents)\n",
    "        probability.append(len(doc_ids_1)/totl)  \n",
    "    \n",
    "    # Reshape and normalize\n",
    "    probability = np.asarray(probability).reshape(len(probability),1)\n",
    "    frequency = np.asarray(frequency).reshape(len(frequency),1)\n",
    "    frequency = frequency / max(frequency)\n",
    "        \n",
    "    return probability, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_to_doc_class(corpus, min_word_length, readmitted_0):\n",
    "    \n",
    "    # Dictionary: word -> documents ids it appears in (doc ids with class 0, doc ids with class 1)\n",
    "    word_to_doc_class = dict()  \n",
    "    doc_id = 0\n",
    "    for document in corpus:\n",
    "        for w in clean_document(document, min_word_length):\n",
    "            if w not in word_to_doc_class:\n",
    "                # The value equals: ((doc ids at 0), (doc ids at 1))\n",
    "                word_to_doc_class[w] = (set(), set()) \n",
    "            \n",
    "            (doc_ids_0, doc_ids_1) =  word_to_doc_class[w]\n",
    "            if doc_id in readmitted_0:\n",
    "                doc_ids_0.add(doc_id)\n",
    "            else:\n",
    "                doc_ids_1.add(doc_id)\n",
    "        doc_id += 1\n",
    "    \n",
    "    return word_to_doc_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_readmitted_0(data):\n",
    "    \n",
    "    # Extract readmitted column\n",
    "    readmitted = pd.DataFrame(data,columns=['readmitted']).to_numpy().flatten()\n",
    "    \n",
    "    # Extract classes\n",
    "    readmitted_0 = np.argwhere(readmitted == 0).flatten()\n",
    "    \n",
    "    return readmitted_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool functions for Word to Bags (simple or advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "def simple_bag_of_words(filtered_data, vocabulary, min_word_length):\n",
    "    \n",
    "    # Lenghts \n",
    "    nb_documents = len(filtered_data)\n",
    "    nb_vocab_words = len(vocabulary)\n",
    "    \n",
    "    # Compute the occurence scores\n",
    "    bag_of_words = np.zeros((nb_documents,nb_vocab_words))\n",
    "    for i in range(nb_documents):\n",
    "        document = filtered_data[i]\n",
    "        occurence = Counter()\n",
    "        occurence.update(clean_document(document, min_word_length))\n",
    "        \n",
    "        # Format the vector to the vocabulary\n",
    "        for j in range(nb_vocab_words):\n",
    "            word = vocabulary[j]\n",
    "            if word in occurence.keys():\n",
    "                bag_of_words[i,j] = 1\n",
    "            else:\n",
    "                bag_of_words[i,j] = 0\n",
    "             \n",
    "    return np.asarray(bag_of_words).reshape((nb_documents,nb_vocab_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def advanced_bag_of_words(filtered_data, global_vocabulary, max_ngram_size):\n",
    "    \n",
    "    # Number of documents \n",
    "    nb_documents = len(filtered_data)\n",
    "    \n",
    "    # Compute the TFIDF scores \n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,max_ngram_size))\n",
    "    fitted_vectorizer = tfidf_vectorizer.fit(filtered_data)\n",
    "    tfidf_vectorizer_vectors = fitted_vectorizer.transform(filtered_data).toarray()\n",
    "    local_vocabulary = fitted_vectorizer.get_feature_names()\n",
    "    \n",
    "    # Format the vector to the vocabulary\n",
    "    bag_of_words = np.zeros((nb_documents, 1))\n",
    "    start = True \n",
    "    for word in global_vocabulary:\n",
    "        if word in set(local_vocabulary):\n",
    "            index_tfidf = local_vocabulary.index(word)\n",
    "            score_tfidf = np.asarray(tfidf_vectorizer_vectors[:,index_tfidf]).reshape(nb_documents, 1)\n",
    "        else:\n",
    "            score_tfidf = np.zeros((nb_documents, 1))\n",
    "            \n",
    "        if start:\n",
    "            bag_of_words = score_tfidf\n",
    "            start = False\n",
    "        else:\n",
    "            bag_of_words = np.concatenate((bag_of_words, score_tfidf), axis=1)\n",
    "    \n",
    "    return bag_of_words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tool functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "def load_csv_file(filename, is_numpy):\n",
    "    \n",
    "    if is_numpy:\n",
    "        return np.genfromtxt(filename, delimiter=',')\n",
    "    else:\n",
    "        with open(filename, 'r') as read_obj:\n",
    "            csv_reader = reader(read_obj)\n",
    "            return list(csv_reader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_score(score_1, score_2, score_3):\n",
    "    \n",
    "    score = np.concatenate((score_1, score_2), axis=1)\n",
    "    score = np.concatenate((score, score_3), axis=1)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_list_to_file(list_data, list_filename, is_numpy):\n",
    "    \n",
    "    if is_numpy:\n",
    "        np.savetxt(list_filename, list_data, delimiter=\",\")\n",
    "    else:\n",
    "        with open(list_filename,'w', newline='') as myfile:\n",
    "            wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "            wr.writerow(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_nlp(train_data, test_data, is_advanced_vocabulary, is_advanced_bag_of_words, min_freq_occurence, \n",
    "             max_freq_occurence, min_word_length, max_ngram_size, diag_number, \n",
    "             threshold_proba_0, threshold_proba_1):\n",
    "    \n",
    "    # Construc the vocubalary (simple or advanced)\n",
    "    if is_advanced_vocabulary:\n",
    "        # Construct advanced vocabulary using train data\n",
    "        vocabulary = construct_advanced_vocabulary(train_data, threshold_proba_0, threshold_proba_1, min_freq_occurence, max_freq_occurence, diag_number, min_word_length)\n",
    "        \n",
    "    else:\n",
    "        # Construct simple vocabulary using train data\n",
    "        vocabulary = construct_simple_vocabulary(train_data, min_freq_occurence, max_freq_occurence, diag_number, min_word_length)\n",
    "\n",
    "    # Use constructed vocabulary to filter the train and test data\n",
    "    filtered_train_data = extract_and_filter_corpus_from_vocabulary(train_data, vocabulary, diag_number, min_word_length)\n",
    "    filtered_test_data = extract_and_filter_corpus_from_vocabulary(test_data, vocabulary, diag_number, min_word_length)\n",
    "    \n",
    "    # Construct the train/test data using Bag of Words (simple or advanced)\n",
    "    if is_advanced_bag_of_words:\n",
    "        # Constructe the TFIDF vectors from filtered data using bag of words\n",
    "        score_train_data = advanced_bag_of_words(filtered_train_data, vocabulary, max_ngram_size)\n",
    "        score_test_data = advanced_bag_of_words(filtered_test_data, vocabulary, max_ngram_size)\n",
    "    else:\n",
    "        # Constructe simple Bag of Words\n",
    "        score_train_data = simple_bag_of_words(filtered_train_data, vocabulary, min_word_length)\n",
    "        score_test_data = simple_bag_of_words(filtered_test_data, vocabulary, min_word_length)\n",
    "        \n",
    "    \n",
    "    return vocabulary, score_train_data, score_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the NLP are:\n",
    "    \n",
    "\"train_data\" = The train dataset\n",
    "\n",
    "\"test_data\" = The test dataset \n",
    "\n",
    "\"min_freq_occurence\" = Filters out words appearing less than \"min_freq_occurence\" (frequency between 0 and 1)\n",
    "\n",
    "\"max_freq_occurence\" = Filters out words appearing less than \"max_freq_occurence\" (frequency between 0 and 1)\n",
    "\n",
    "\"train_data_filename\" = Output train data file name \n",
    "\n",
    "\"test_data_filename\" = Output test data file name\n",
    "\n",
    "\"output_vocabulary_filename\" = Output vocabulary file name \n",
    "\n",
    "\"mode\" = 0 if we take {diag_1, diag_2, diag_3} combined to do the NLP or 1,2 or 3 if make an NLP on each diag individually \n",
    "\n",
    "\"min_word_length\" = Filters out words less than \"min_word_length\"\n",
    "\n",
    "\"max_ngram_size\" = Maximum size of the ngrams used in Word to Bags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp(train_data, test_data, min_freq_occurence, max_freq_occurence, train_data_filename,\n",
    "                         test_data_filename, output_vocabulary_filename, \n",
    "                            is_corpus_combined, is_advanced_vocabulary, is_advanced_bag_of_words, min_word_length, max_ngram_size,\n",
    "                               threshold_proba_0, threshold_proba_1):\n",
    "    \n",
    "    \n",
    "    # Run NLP in 2 different ways: the diagnostic combined or not \n",
    "    if is_corpus_combined:\n",
    "        # Run NLP on all data sets and get train/test scores\n",
    "        (vocabulary, score_train_data, score_test_data) = core_nlp(train_data, test_data, is_advanced_vocabulary, is_advanced_bag_of_words, min_freq_occurence, max_freq_occurence, min_word_length, max_ngram_size, 0, threshold_proba_0, threshold_proba_1)\n",
    "        \n",
    "        # Load vocabulary memory \n",
    "        write_list_to_file(vocabulary, output_vocabulary_filename, False)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Run NLP on first diagnostic and get train/test scores\n",
    "        (vocabulary_1, score_train_data_1, score_test_data_1) = core_nlp(train_data, test_data, is_advanced_vocabulary, is_advanced_bag_of_words, min_freq_occurence, max_freq_occurence, min_word_length, max_ngram_size, 1, threshold_proba_0, threshold_proba_1)\n",
    "        # Run NLP on second diagnostic and get train/test scores\n",
    "        (vocabulary_2, score_train_data_2, score_test_data_2) = core_nlp(train_data, test_data, is_advanced_vocabulary, is_advanced_bag_of_words, min_freq_occurence, max_freq_occurence, min_word_length, max_ngram_size, 2, threshold_proba_0, threshold_proba_1)\n",
    "        # Run NLP on third diagnostic and get train/test scores\n",
    "        (vocabular_3, score_train_data_3, score_test_data_3) = core_nlp(train_data, test_data, is_advanced_vocabulary, is_advanced_bag_of_words, min_freq_occurence, max_freq_occurence, min_word_length, max_ngram_size, 3, threshold_proba_0, threshold_proba_1)\n",
    "        # Combine the obtained scores\n",
    "        score_train_data = combine_score(score_train_data_1, score_train_data_2, score_train_data_3)\n",
    "        score_test_data = combine_score(score_test_data_1, score_test_data_2, score_test_data_3)\n",
    "            \n",
    "        \n",
    "    # Load train/test scores into memory \n",
    "    write_list_to_file(score_train_data, train_data_filename, True)\n",
    "    write_list_to_file(score_test_data, test_data_filename, True)   \n",
    "    \n",
    "    # Check that output vectors have same length\n",
    "    print(\"Shape of the train and test datasets\")\n",
    "    print(score_train_data.shape)\n",
    "    print(score_test_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
