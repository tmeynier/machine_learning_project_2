{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipynb.fs.full.nlp import clean_document\n",
    "from ipynb.fs.full.nlp import extract_and_combine \n",
    "from ipynb.fs.full.nlp import compute_readmitted_0\n",
    "from ipynb.fs.full.nlp import compute_word_to_doc_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_scatter_points_position(word_to_doc_class, frequency_scale, largest_vocabulary, nb_documents):\n",
    "    \n",
    "    # Compute X=probabilty and Y=frequency scatter points\n",
    "    probability = list()\n",
    "    frequency = list()\n",
    "    for doc_ids_0, doc_ids_1 in word_to_doc_class.values():\n",
    "        \n",
    "        totl = len(doc_ids_0) + len(doc_ids_1) \n",
    "        # Use log scale for better display\n",
    "        frequency.append(math.log(1 + frequency_scale*totl/nb_documents))\n",
    "        probability.append(len(doc_ids_1)/totl)  \n",
    "    \n",
    "    # Reshape and normalize\n",
    "    probability = np.asarray(probability).reshape(len(probability),1)\n",
    "    frequency = np.asarray(frequency).reshape(len(frequency),1)\n",
    "    frequency = frequency / max(frequency)\n",
    "    \n",
    "    return probability, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scatter_points_colors(largest_vocabulary, bow_vocabulary):\n",
    "    \n",
    "    # Compute fill and border colors of scatter points \n",
    "    color = list()\n",
    "    border_color = list()\n",
    "    doc_id = 0\n",
    "    for word in largest_vocabulary:\n",
    "        if word not in bow_vocabulary:\n",
    "            color.append('#efefef')\n",
    "            border_color.append('#%02x%02x%02x' % (128, 128, 128))\n",
    "        else:\n",
    "            color.append('#%02x%02x%02x' % (0, 200, 0))\n",
    "            border_color.append('#%02x%02x%02x' % (0, 200, 0))\n",
    "\n",
    "        doc_id += 1\n",
    "                                  \n",
    "    return color, border_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bokeh Libraries\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_file\n",
    "from bokeh.models import ColumnDataSource, NumeralTickFormatter, Label, LabelSet\n",
    "\n",
    "def animation_plot(frequency, probability, labels, color, border_color, per_readmitted_0):\n",
    "    \n",
    "    # Output to file\n",
    "    output_file('visualization.html',\n",
    "                title='visualization')\n",
    "\n",
    "    # Store the data in a ColumnDataSource\n",
    "    source = ColumnDataSource(data=dict(frequency = frequency,\n",
    "                                        probability = probability,\n",
    "                                        labels = labels,\n",
    "                                        color = color,\n",
    "                                        border_color = border_color))\n",
    "\n",
    "    # Specify the selection tools to be made available\n",
    "    select_tools = [\"hover\",\"crosshair\",\"pan\",\"wheel_zoom\",\"zoom_in\",\"zoom_out\",\"box_zoom\",\n",
    "                    \"undo\",\"redo\",\"reset\",\"tap,save\",\"box_select\",\"poly_select\",\"lasso_select\"]\n",
    "\n",
    "    # Create the figure\n",
    "    fig = figure(plot_height=800,\n",
    "                 plot_width=1500,\n",
    "                 x_axis_label=\"Number of documents classed 1 against all the documents in which the word appears\",\n",
    "                 y_axis_label=\"Percentage of documents in which word appears\",\n",
    "                 title='Animation for Word to Bag output with a display of the goof features',\n",
    "                 toolbar_location='below',\n",
    "                 tools=select_tools, \n",
    "                 tooltips=\"@labels\",\n",
    "                 background_fill_color='#efefef')\n",
    "\n",
    "    # Add Legend \n",
    "    fig.circle(0.7, 0.8, legend=\"In NLP vocabulary\", color='#%02x%02x%02x' % (0, 150, 0), \n",
    "               line_color = '#%02x%02x%02x' % (0, 150, 0), line_width = 2, fill_alpha=0.4)\n",
    "    fig.circle(0.7, 0.6, legend=\"Not in NLP vocabulary\", color='#efefef', \n",
    "               line_color = '#%02x%02x%02x' % (128, 128, 128), line_width = 2, fill_alpha=0.4)\n",
    "\n",
    "    # add a line at proba 0.5\n",
    "    fig.line([0.5, 0.5], [0, 1], line_width=3, color=\"red\", line_dash='dashed')\n",
    "\n",
    "    # add a line admitted 1\n",
    "    fig.line([0.5, 1], [1-per_readmitted_0, 1-per_readmitted_0], line_width=3, color=\"red\", line_dash='dashed')\n",
    "\n",
    "    # add a line admitted 0\n",
    "    fig.line([0, 0.5], [per_readmitted_0, per_readmitted_0], line_width=3, color=\"red\", line_dash='dashed')\n",
    "\n",
    "    # add a circle admitted 1\n",
    "    fig.circle(1, 1-per_readmitted_0, size=20, color=\"red\", alpha=0.3, line_color= \"red\", line_width=3)\n",
    "\n",
    "    # add a circle admitted 0\n",
    "    fig.circle(0, per_readmitted_0, size=20, color=\"red\", alpha=0.3, line_color= \"red\", line_width=3)\n",
    "\n",
    "    # add a line at proba 0.5\n",
    "    fig.line([0.5, 0.5], [0, 1], line_width=3, color=\"blue\", line_dash='dashed')\n",
    "\n",
    "    # Format the y-axis tick labels as percentages\n",
    "    fig.yaxis[0].formatter = NumeralTickFormatter(format='00.0%')\n",
    "\n",
    "    # Add square representing each player\n",
    "    fig.scatter(x='probability', radius=0.01,\n",
    "               y='frequency',\n",
    "               source=source,\n",
    "               fill_color='color',\n",
    "               selection_color='deepskyblue',\n",
    "               line_color = 'border_color',\n",
    "               line_width = 2,\n",
    "               fill_alpha=0.4)\n",
    "    # Visualize\n",
    "    show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tool function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "def load_csv_file(filename, is_numpy):\n",
    "    \n",
    "    if is_numpy:\n",
    "        return np.genfromtxt(filename, delimiter=',')\n",
    "    else:\n",
    "        with open(filename, 'r') as read_obj:\n",
    "            csv_reader = reader(read_obj)\n",
    "            return list(csv_reader)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main visualization function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(bow_vocabulary_filename, bow_score_filename, data, min_word_length, frequency_scale,\n",
    "                 radius_scale, display_threshold_0, display_threshold_1):\n",
    "    \n",
    "    # Get the vocabulary and the scores used by word to bags \n",
    "    bow_vocabulary = load_csv_file(bow_vocabulary_filename, False)\n",
    "            \n",
    "    # Get the training scores of word to bag \n",
    "    bow_scores = load_csv_file(bow_score_filename, True)\n",
    "    \n",
    "    # Compute percentage readmitted\n",
    "    readmitted_0 = compute_readmitted_0(data)\n",
    "    \n",
    "    # Get the corpus \n",
    "    corpus = extract_and_combine(data, 0)\n",
    "    \n",
    "    # Number of ducuments \n",
    "    nb_documents = len(corpus)\n",
    "    \n",
    "    # Compute the largest possible vocabulary possibly used, \n",
    "    # with its word frequencies (max 1 count per document)\n",
    "    word_to_doc_class = compute_word_to_doc_class(corpus, min_word_length, readmitted_0)\n",
    "    largest_vocabulary = list(word_to_doc_class.keys())\n",
    "           \n",
    "    # Compute how efficient is each possible word = scatter_point\n",
    "    (probability, frequency) = compute_scatter_points_position(word_to_doc_class, \n",
    "                                                               frequency_scale, largest_vocabulary, nb_documents)\n",
    "    \n",
    "    # Colors scatter points depending on largest_vocabulary\n",
    "    (color, border_color) = compute_scatter_points_colors(largest_vocabulary, bow_vocabulary)\n",
    "    \n",
    "    # Animation plot of the comparison of the two vocabularies\n",
    "    percentage_readmitted_0 = 1.0*len(readmitted_0)/nb_documents\n",
    "    animation_plot(frequency, probability, largest_vocabulary,color, border_color, percentage_readmitted_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
