{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "articles = {}\n",
    "\n",
    "for dirpath, subdirs, files in os.walk('/kaggle/input'):\n",
    "    for x in files:\n",
    "        if x.endswith(\".json\"):\n",
    "            articles[x] = os.path.join(dirpath, x)        \n",
    "metadata = pd.read_csv('/kaggle/input/CORD-19-research-challenge/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literature = []\n",
    "for index, row in tqdm(metadata.iterrows(), total=metadata.shape[0]):\n",
    "    sha = str(row['sha'])\n",
    "    if sha != 'nan':\n",
    "        sha = sha + '.json';\n",
    "        try:\n",
    "            with open(articles[sha]) as f:\n",
    "                data = json.load(f)\n",
    "                key = 'abstract'\n",
    "                abstract = \"\"\n",
    "                if key in data:\n",
    "                    for content in data[key]:\n",
    "                        abstract += content['text']\n",
    "                text = \"\"\n",
    "                if \"body_text\" in data:\n",
    "                    for content in data[\"body_text\"]:\n",
    "                        text += content['text']\n",
    "                literature.append({'file': articles[sha], 'abstract': abstract, 'content': text}) \n",
    "        \n",
    "        except KeyError:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scispacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    raw = raw.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words=[]\n",
    "    for j in range(0,len(tokens)):\n",
    "        words.append([ i for i in tokens[j] if not i in stop_words])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector(doc):\n",
    "    return nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for article in tqdm(literature):\n",
    "    vec = to_vector(article[\"abstract\"])\n",
    "    vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"covid 19 sars pneumonia covid-19 coronavirus medical care surge capacity and nursing homes  allocation of scarce resources personal protective equipment ppe disease management  processes of care clinical characterization and management of the virus\"\n",
    "vec_question = to_vector(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_question.has_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = []\n",
    "for vec in tqdm(vectors):\n",
    "    if vec.has_vector:\n",
    "        similarity.append(vec.similarity(vec_question))\n",
    "similarity = np.array(similarity)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors[np.argmax(similarity)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "ind = heapq.nlargest(1000, range(len(similarity)), similarity.take)\n",
    "#similarity[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ind:\n",
    "    print(vectors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_care_articles = np.array(literature)[np.array(ind)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt((\"indices_medical_care.csv\"), np.array(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_care_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts= []\n",
    "for article in medical_care_articles:\n",
    "    vec = article[\"content\"]\n",
    "    texts.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, RegexpTokenizer,PunktSentenceTokenizer, sent_tokenize\n",
    "\n",
    "Interest=[]\n",
    "for text in texts:\n",
    "    raw = text\n",
    "\n",
    "    #Make everything lower case - useful for stop-words\n",
    "    raw = raw.lower()\n",
    "    #print(raw)\n",
    "\n",
    "    # split into sentences. important not to get whitespace\n",
    "    sentences = sent_tokenize(raw)\n",
    "    #print('Tokenised sentences',sentences)\n",
    "\n",
    "    # remove punctuation - this can create problems with '-' words\n",
    "    import string\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    nopunk=[w.translate(table) for w in sentences]\n",
    "    #print('Removed Punctuation',nopunk)\n",
    "\n",
    "    #Tokenise words\n",
    "    tokens=[]\n",
    "    for sentence in nopunk:\n",
    "        tokens.append(nltk.word_tokenize(sentence))\n",
    "    #print('Tokenised sentences',tokens)\n",
    "\n",
    "    #Remove Stop Words. Remember add stopwords that may be relevant\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words=[]\n",
    "    for j in range(0,len(tokens)):\n",
    "        words.append([ i for i in tokens[j] if not i in stop_words])\n",
    "    #print ('Removed stop words',words)\n",
    "\n",
    "    # CONSIDER COMMENTING: stemming of words\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = []\n",
    "    for j in range(0,len(words)):\n",
    "        stemmed.append([porter.stem(word) for word in words[j]])\n",
    "    #print('Post stemming',stemmed)\n",
    "\n",
    "    #CAN COMMENT BUT LESS CRITICAL: Lemmatisation (NOT of stemmed but could change to combine both)\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    lemmed = []\n",
    "    for j in range(0,len(words)):\n",
    "        lemmed.append([lemmatizer.lemmatize(word) for word in words[j]])\n",
    "    #print('Post Lemmatisation',lemmed)\n",
    "    \n",
    "    Interest.append(lemmed)\n",
    "print('List of Tokenised abstracts',Interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for article in tqdm(medical_care_articles):\n",
    "    text = article[\"content\"]\n",
    "    sentences += sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sentences = []\n",
    "for sentence in tqdm(sentences):\n",
    "    vec_sentences.append(to_vector(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)\"\n",
    "vec_question = to_vector(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_sen = []\n",
    "for vec in tqdm(vec_sentences):\n",
    "    if vec.has_vector:\n",
    "        similarity_sen.append(vec.similarity(vec_question))\n",
    "similarity_sen = np.array(similarity_sen)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2 = heapq.nlargest(10, range(len(similarity_sen)), similarity_sen.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in ind2:\n",
    "    res.append(vec_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[question] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "second = {}\n",
    "for key in results.keys():\n",
    "    j = []\n",
    "    for i in results[key]:\n",
    "        #print(type(str(i)))\n",
    "        j.append(str(i))\n",
    "    second[str(key)] = j\n",
    "\n",
    "json_ = json.dumps(second)\n",
    "print(json_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.json', 'w') as fp:\n",
    "    json.dump(second, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
